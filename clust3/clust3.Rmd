---
title: 'Кластерный анализ: оценка качества кластеризации'
author: "Алла Тамбовцева"
date: '30 апреля 2018 г '
output:
  html_document: default
---

На этом занятии мы попробуем сымитировать небольшое исследование, основанное на кластерном анализе: реализуем иерархический кластерный анализ, проверим качество получившейся кластеризации, определим, сколько кластеров все-таки выбрать и проведем кластеризацию, используя метод k-средних (*k-means*).

### Данные

Загрузим базу данных `reg_elect.csv`, содержащую результаты президентских выборов 2018 года:

```{r, echo = FALSE}
df <- read.csv("reg_elect.csv")
```

```{r, eval = FALSE}
df <- read.csv(file.choose())
```

Посмотрим на нее:

```{r, message = FALSE, warning=FALSE}
View(df)
```

```{r}
head(df)
```

В базе данных 87 наблюдений, так как в ней помимо регионов сохранены результаты выборов на территории за пределами РФ и в городе Байконур. 

**Пояснения по переменным:** 

* total -- общее число зарегистрированных избирателей
* invalid -- число недействительных бюллетеней
* valid -- число действительных бюллетеней
* turnout -- явка (сумма действительных и недействительных бюллетеней)
* далее -- число голосов за каждого кандидата (по фамилиям)
* столбцы с `_perc` -- явка в процентах и проценты голосов за кандидатов

### Иерархический кластерный анализ

Выберем переменные, на основе которых мы будем кластеризовать регионы. Нам понадобятся все столбцы, которые заканчиваются на `_perc`: явка в процентах и проценты голосов за кандидатов. Воспользуемся удобной функцией `ends_with` из библиотеки `dplyr`.

```{r, message = FALSE, warning=FALSE}
library(dplyr)
to_clust <- df %>% select(ends_with("_perc"))
```

**Пояснения к коду:** логика использования оператора `%>%` такая: взять то, что слева, и подать этот объект на вход функции, которая стоит справа от оператора. В нашем случае мы берем базу `df` и подаем ее на вход функции `select` для выбора столбцов. Внутри скобок у `select` мы могли бы через запятую перечислить нужные нам столбцы, но сейчас мы поступим более хитро: выберем все столбцы, названия которых заканчиваются на `_perc`. Выбранные столбцы сохраним в новую маленькую базу `to_clust`. 

Назовем строки по названиям региона (да, можно было бы вместо длинных названий создать числовой *id* регионов, но оставим названия для наглядности, чтобы не тратить время на "расшифровку" дендрограммы).

```{r}
rownames(to_clust) <- df$region
```

Теперь все готово к работе. Как всегда, стандартизируем данные (`scale`) и создадим матрицу расстояний `m`.

```{r}
m <- dist(scale(to_clust))
```

Проведем кластерный анализ, используя метод Варда в качестве метода агрегирования (да, по правилам нам нужна матрица с квадратами евклидова расстояния, но давайте пока оставим матрицу с обычным евклидовым расстоянием, чтобы можно было использовать ту же матрицу для другого метода агрегирования). 

```{r}
hc <- hclust(m, method = "ward.D")
```

**Примечание:** Вместо того, чтобы возводить значения матрицы расстояний в квадрат, можно взять исходную матрицу `m` и использовать метод `ward.D2`, который сам возводит значения расстояний из матрицы расстояний `m` в квадрат. Другими словами, код `hclust(m^2, method = "ward.D")`и код `hclust(m, method = "ward.D2")` должны дать одинаковые результаты. 

Посмотрим на дендрограмму:

```{r, fig.height= 10, fig.width=20}
plot(hc, cex = 0.9)
```

Давайте, чтобы не делать слишком общую классификацию, выделим 5 кластеров. Может быть, такой выбор окажется неидеальным, но зато будет интереснее оценивать качество кластеризации.

```{r, fig.height= 10, fig.width=20}
plot(hc, cex = 0.9)
rect.hclust(hc, k = 5)
```

Если график просто появляется в окне *Plots*, то у него границы кластеров могут "уходить вверх" из-за того, что подписи наблюдений слишком длинные. Есть альтернатива -- прочертить линию на том уровне (на том расстоянии между кластерами), который нам нужен. По дендрограмме выше видно, что для того, чтобы получить пять кластеров, нужно "разрезать" дендрограмму на высоте примерно 18.

```{r, fig.height= 10, fig.width=20}
plot(hc, cex = 0.9)
abline(h = 18, col = "red") # h - horizontal line, col - color
```

Вытащим из полученной кластеризации метки кластеров. Добавим их отдельным столбцом в базу данных, но перед этим сделаем эти метки факторными, то есть скажем R, что метки 1, 2, 3, 4, 5 -- это не числа, где 5 -- самое большое, а условные обозначения, коды (как в случае, когда мы кодируем рспондентов мужского пола цифрой 2, а женского -- цифрой 1).

```{r}
groups5 <- cutree(hc, k = 5)
df$groups5 <- factor(groups5)
```

### Оценка кластеризации: содержательно

Теперь посмотрим на каждый кластер в отдельности -- будем выбирать из базы строки, где значения `groups5` равны то 1, то 2, и так далее, а потом оценивать содержательно, насколько разумными у нас получились кластеры. Воспользуемся функцией `filter()` из библиотеки `dplyr`. 

```{r, message=FALSE, warning=FALSE}
df %>% filter(groups5 == 1) %>% View
df %>% filter(groups5 == 2) %>% View
df %>% filter(groups5 == 3) %>% View
df %>% filter(groups5 == 4) %>% View
df %>% filter(groups5 == 5) %>% View
```

В первом кластере оказались регионы, которые относительно близки географически (Дальний Восток, Сибирь) и в которых достаточно высокий процент за Грудинина (КПРФ). Второй кластер тоже получился достаточно "географичным": в основном, в нем находятся северные регионы и регионы Центральной России. В этих регионах наблюдается относительно высокий процент у Жириновского и у Собчак. В третьем кластере находятся южные регионы и регионы так называемого "красного пояса". В этих регионах высока поддержка Путина и Грудинина. В четвертом кластере находятся всего пять наблюдений: регионы с запредельно высокой явкой и процентом голосов за Путина (Москва, Санкт-Петербург, Чеченская республика, Республика Ингушетия) и территория за пределами РФ. В пятом кластере характеристики регионов похожи на характеристики элементов предыдущего кластера, но значения явки и процента за Путина пониже.

### Оценка кластеризации: визуально

Теперь давайте посмотрим на описательные статистики по группам (кластерам), точнее, на средние значения разных переменных по группам. Сначала сгруппируем данные по переменной `groups5`, то есть по кластерам. Потом запросим описательные статистики по конкретным переменным (общая функция для описательных статистик -- `summarise`, но мы используем `summarise_at`, так как нас интересуют определенные столбцы). Перечисляем нужные переменные в `.vars = vars()`, а нужные статистики в `.funs = funs()`. Дальше хотим посмотреть на полученную таблицу в отдельном окне -- `View`.

```{r}
df %>% group_by(groups5) %>% 
  summarise_at(.vars = vars(ends_with("_perc")), .funs = funs(mean)) %>% 
  View
```

Видно, что средние значения показателей по группам отличаются.

А теперь посмотрим на распределения разных переменных по кластерам и проверим, правда ли, что они отличаются. Начнем с явки. Построим "ящики с усами" -- для этого нам понадобится библиотека `ggplot2`. 

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
ggplot(data = df, aes(x = "", y = turnout_perc)) + geom_boxplot() + facet_grid(~groups5)
```

**Пояснения к коду:** Графики, построенные с помощью `ggplot2`, создаются по слоям. Сначала в "главном" слое указывается база данных, из которой нужно выбирать переменные (`data = df`), затем в `aes()` указываются переменные, значения которых будут идти по осям `x` и `y`. Далее следует слой с типом графика (`geom_boxplot`) и слой, который позволит нарисовать графики для каждой группы в отдельном окне-фасетке (`facet_grid`).

Как можно заметить, распределения явки в разных кластерах совсем не похожи друг на друга: отличаются не только медианные значения, но и разброс значений. Однако стоит помнить, что "ящики с усами" наиболее информативны в случае, когда распределение данных нормальное или близко к нормальному. 
Посмотрим теперь на скрипичные диаграммы (*violin plots*) для явки (в `aes()` мы добавили `fill = groups5`, чтобы графики были разноцветными, в зависимости от кластера):

```{r}
ggplot(data = df, aes(x = "", y = turnout_perc, fill = groups5)) + geom_violin() + facet_grid(~groups5)
```

Картина стала еще более разнообразной. На скрипки, правда, графики особо не похожи, но зато они позволяют увидеть интересные особенности распределений. Так, в первом кластере распределение явки скошено вправо ("хвост" распределения справа), значения явки сконцентрированы на отрезке от 55% до 65%. Во втором кластере "хвостов" у распределения почти нет, значения явки сосредоточены на участке от 65% до 70%. В третьем кластере распределение явки очень растянутое, широкое, есть регионы с явкой 90% и, возможно, выше. В четвертом кластере распределение явки очень интересное, похоже на равномерное: доля регионов с явкой от 60% до 70% примерно такая же, как доля регионов с явкой от 70% до 80%. В пятом кластере находятся регионы с очень высокой явкой, более 85%, причем преобладают регионы с явкой 90%-95%.

**Примечание:** интерпретация -- это хорошо, но не забывайте про здравый смысл: в четвертом и пятом кластерах всего 5 регионов, поэтому все выражения "преобладают регионы" и "большинство регионов в кластере" очень условны, поскольку общее число регионов очень мало.

Для тех, кто любит менее экстравагантные графики: можно просто построить гистограммы:

```{r}
# fill - цвет заливки
# col - цвет границ графика, общий для всех кластеров
# bins - число интервалов (столбцов) в гистограмме
ggplot(data = df, aes(x = turnout_perc , fill = groups5)) + geom_histogram(bins = 6, col = "black") + facet_grid(~groups5)
```

Конечно, для того, чтобы делать какие-то выводы, мало сравнить распределения явки, нужно посмотреть и на другие показатели. Рассматривать проценты за всех кандидатов мы сейчас не будем, можно подставить интересующие переменные в код для графиков, посмотрим только на Грудинина, кандидата от КПРФ, и Путина:

```{r}
ggplot(data = df, aes(x = "", y = Grudinin_perc, fill = groups5)) + geom_violin() + facet_grid(~groups5)
```

В первом кластере сосредоточены регионы с относительно высоким процентом за Грудинина. Во втором кластере процент за кандидата более низкий и "неровный", в третьем -- похожая ситуация, только разброс значений выше. В четвертом кластере находятся регионы, менее склонные голосовать за Грудинина, распределение скошено вправо, больше районов, где процент за этого кандидата около 5. В пятом кластере распределение очень специфическое. В целом, процент за Грудинина в этом регионе очень маленький, но при этом есть нетипичные регионы с процентом повыше.

И, наконец, графики с процентами голосов за Путина:

```{r}
ggplot(data = df, aes(x = "", y = Putin_perc, fill = groups5)) + geom_violin() + facet_grid(~groups5)
```

Картина получилась обратная тому, что мы видели на графиках для Грудинина, но похожая на то, что мы видели на графиках для явки. Можем посмотреть на распределение остальных показателей и получить "профиль" каждого кластера: например, кластер 1 содержит регионы с относительно низкой явкой и низким процентом за Путина, но высоким процентом за Грудинина, и так далее.

Теперь посмотрим на диаграммы рассеяния (это мы уже делали на самой первой лекции, только по показателям Всемирного банка).

```{r}
ggplot(data = df, aes(x = turnout_perc, y = Putin_perc)) + geom_point(aes(color = groups5)) 
ggplot(data = df, aes(x = Grudinin_perc, y = Putin_perc)) + geom_point(aes(color = groups5)) 
```

В целом, точки сгруппированы по цветам, то есть, по кластерам.

### Оценка кластеризации: формально

Пока мы исследовали распределения только визуально. Давайте теперь применим какой-нибудь формальный критерий. Так как у нас пять групп, и понятно, что распределение разных показателей по группам не является нормальным, применим критерий Краскела-Уоллиса. В качестве примера возьмем явку и проценты за Грудинина и Путина.

```{r}
kruskal.test(df$turnout_perc ~ df$groups5)
kruskal.test(df$Grudinin_perc ~ df$groups5)
kruskal.test(df$Putin_perc ~ df$groups5)
```

Нулевая гипотеза состоит в том, что данные в выборках взяты из одного распределения (медианы распределений равны). Так как p-value близко к 0, на уровне значимости 5% можно отвергнуть эту гипотезу: распределения явки/процентов за Грудинина/процентов за Путина отличаются по кластерам (медианы этих показателей по разным кластерам не равны).

### Оценка кластеризации: валидация

Мы говорили о том, что результаты кластерного анализа, полученные с использованием одного расстояния/метода агрегирования можно сравнивать с реализацией кластерного анализа, полученного с помощью другого расстояния/метода агрегирования. Сравним результаты нашей реализации (метод Варда) с результатами кластерного анализа методом средней связи, используя индекс Ранда ([Rand Index](https://en.wikipedia.org/wiki/Rand_index)).

```{r}
# groups5 - наши метки кластеров
# groups5_2 - метки по результатам КА с методом средней связи
hc2 <- hclust(m, method = "average")
groups5_2 <- cutree(hc2, k = 5)
```

Установим и загрузим библиотеку `fossil`, в которой хранится функция для расчета индекса Ранда.

```{r, eval = FALSE}
install.packages("fossil")
```

Посчитаем сам индекс:

```{r, message=FALSE, warning= FALSE}
library(fossil)
rand.index(groups5, groups5_2)
```

Значение 0.4 - достаточно невысокое. Но выводы о том, что кластеризация получилась не очень, делать еще рано. Все-таки у метода средней связи и метода Варда разные особенности.

Еще показатели согласованности результатов кластерного анализа можно получить с помощью библиотеки `fpc`. Установим ее:

```{r, eval=FALSE}
install.packages("fpc")
```

Обратимся к библиотеке и запросим все возможные статистики:

```{r}
library(fpc)
# m - матрица расстояний
cluster.stats(m, groups5, groups5_2)
```

Функция `cluster.stats()` выдает целый ряд разных показателей, почитать про них можно в документации, вызвав *help* через `?cluster.stats()`. 

### Оценка кластеризации: "нужные" кластеры vs случайный шум

Проверим, правда ли кластеры, которые мы видели на дендрограмме, получились обосновано, исходя из данных, а не случайно, из-за нескольких выбросов или какого-то шума в данных. Для этого нам потребуется [библиотека](http://stat.sys.i.kyoto-u.ac.jp/prog/pvclust/) `pvclust` (*pv - от p-value*). 

```{r, eval = FALSE}
install.packages("pvclust")
```

Функция `pvclust` реализует кластерный анализ по столбцам, а не по строкам таблицы, поэтому таблицу сначала нужно транспонировать (`t` перед `to_clust`). 

**Внимание:** исполнение следующего кода займет несколько минут, так как будет запускаться процедура бутстраппирования (*bootstrap*), которая заключается в повторяющемся "выдергивании" подвыборок из исходной выборки и оценивании параметров получившегося распределения.

```{r, message=FALSE, warning=FALSE}
library(pvclust)
fit <- pvclust(t(to_clust), method.hclust = "ward", method.dist = "euclidean")
```

А теперь посмотрим на график. 

```{r, fig.height= 10, fig.width=20}
plot(fit, cex = 0.9) 
```

На дендрограмме над каждым кластером (над каждой "веткой" дендрограммы) подписано p-value -- вероятность того, что кластер образован не случайно (например, из-за наличия в данных каких-то странных значений или шума), а обоснованно, на основе имеющихся показателей (*highly supported by data*). Значения подписаны двумя цветами. Зеленое число -- это почти несмещенное p-value, посчитанное на основе выборок, полученных в результате многошкального бутстраппирования (*multiscale bootstrap resampling*, *AU* -- от *Approximately Unbiased*), красное число -- то же p-value, но посчитанное по итогам обычного, нормального бутстраппирования (*normal bootstrap resampling*, *BP* -- от *Bootstrap Probability*). 

Кроме того, можно выбрать желаемую вероятность -- при которой гипотеза о том, что кластер образован не случайно, не отвергается, и выделить такие кластеры на дендрограмме.

```{r, fig.height= 10, fig.width=20}
plot(fit, cex = 0.7)
pvrect(fit, alpha = 0.95)
```

Если брать вероятность, равную 95%, то видно, сколько кластеров не случайны. Полученная дендрограмма, кстати, отличается от того, что мы видели в начале. Давайте думать дальше.

### Сколько кластеров выбрать?

Конечно, главное правило по выбору числа кластеров: выбрать столько, сколько можно интерпретировать содержательно. Но посмотрим на статистические методы. Для этого нам потребуется библиотека `factoextra`. Установим его:

```{r, eval = FALSE}
install.packages("factoextra")
```

**Elbow method** ("метод согнутого колена", он же "метод каменистой осыпи"). Построим график, где по оси абсцисс отмечено число кластеров $k$, а по оси ординат -- значения функции $W(k)$, которая определяет внутригрупповой разброс в зависимости от числа кластеров.

```{r, message=FALSE, warning=FALSE}
library(factoextra)
fviz_nbclust(to_clust, kmeans, method = "wss") +
  labs(subtitle = "Elbow method")
```

По графику видно, что "колено сгибается" при $k=4$. Наверное, четыре кластера, лучше, чем пять. Но вообще, резкий изгиб наблюдается уже при $k=2$ - вопрос в том, достаточно ли нам только двух кластеров, или мы все же хотим более детальное деление на группы.

```{r, message=FALSE, warning=FALSE}
# geom_vline - добавить вертикальную линию
fviz_nbclust(to_clust, kmeans, method = "wss") +
  labs(subtitle = "Elbow method") +
  geom_vline(xintercept = 4, linetype = 2)
```

Попробуем другой метод.

**Silhouette method** ("силуэтный метод"). 

```{r, message=FALSE, warning=FALSE}
fviz_nbclust(to_clust, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")
```

И опять мы получаем только два кластера. Но тем не менее, кластерный анализ очень сильно зависит от исследователя :)

**Примечание:** что такое "силуэт" и как он считается, см. [здесь](https://en.wikipedia.org/wiki/Silhouette_(clustering)).

### K-means

Вот мы и подошли вплотную к методу *k-средних*. На предыдущем шаге неявно этот метод кластеризации уже был задействован: и Elbow method, и Silhouette method основаны на методе k-средних с разным k.
Давайте попробуем реализовать кластерный анализ методом k-средних, взяв $k=4$.

```{r}
cl <- kmeans(to_clust, 4)
```

**Обратите внимание:** в функции `kmeans()` мы указываем саму базу данных, матрица расстояний `m` нам уже не нужна.

Посмотрим, что внутри `cl`:

```{r}
cl
```

Объект `cl` -  это один большой список (*list*), к элементам которого можно обратиться, используя `$`. Например, запросим метки кластеров (`cluster`) и сохраним их отдельным столбцом в исходную базу данных `df`:

```{r, message=FALSE, warning=FALSE}
df$kmeans4 <- cl$cluster
View(df)
```

Что можно сделать напоследок? Можно реализовать кластерный анализ, используя метод Варда, но выбрав 4 кластера, а потом сравнить результаты с k-means при $k=4$.

```{r}
# hc - результат кластеризации, который мы получили в самом начале
# rand.index - из библиотеки fossil
groups4 <- cutree(hc, k = 4)
rand.index(groups4, cl$cluster)
```

Согласованность результатов уже получше!

А что будет, если поверить графикам, и взять два кластера, как рекомедовали Месье Silhouette и Мистер Согнутое Колено (только не пишите так в домашних заданиях, пожалуйста!)?

```{r}
groups2 <- cutree(hc, k = 2)
cl2 <- kmeans(to_clust, 2)
rand.index(groups2, cl2$cluster)
```

Серьезных изменений в согласованности не произошло. Так что, действовать нужно по велению исследовательского сердца:)

### Дополнение: NbClust

А для тех, кто еще не устал от кластерного анализа, можно попробовать процедуру `Nbclust`, которая включает 30 разных методов нахождения оптимального числа кластеров, и выдает результат, за который проголосовали большинство методов (*majority rule*).

**Внимание:** не увлекайтесь, процедура достаточно специфичная и не всегда дает стабильные результаты! Исполнение кода займет некоторое время.

```{r, eval=FALSE}
# поставить библиотеку
install.packages("NbClust")
```

```{r, message=FALSE, warning=FALSE}
library(NbClust)
res <- NbClust(to_clust, min.nc = 2, max.nc = 8, method = "kmeans")
```

Все результаты (все 30 методов):

```{r}
res$Best.nc
```

Число кластеров, выбраное большинством методов:

```{r}
library(factoextra)
fviz_nbclust(res)
```

Все-таки 2! Или 3 в крайнем случае. Up to you. Решайте из содержательных соображений.
